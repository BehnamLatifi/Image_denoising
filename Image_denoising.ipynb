{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRlHYfbCWvim"
      },
      "source": [
        "import math\n",
        "import cv2\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.feature_extraction import image\n",
        "from keras.layers import  Input,Conv2D,BatchNormalization,Activation,Add,Subtract,MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, CSVLogger\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "\n",
        "\n",
        "def data_aug(img, mode=0):\n",
        "\n",
        "    if mode == 0:\n",
        "        return img\n",
        "    elif mode == 1:\n",
        "        return np.flipud(img)\n",
        "    elif mode == 2:\n",
        "        return np.rot90(img)\n",
        "    elif mode == 3:\n",
        "        return np.flipud(np.rot90(img))\n",
        "    elif mode == 4:\n",
        "        return np.rot90(img, k=2)\n",
        "    elif mode == 5:\n",
        "        return np.flipud(np.rot90(img, k=2))\n",
        "    elif mode == 6:\n",
        "        return np.rot90(img, k=3)\n",
        "    elif mode == 7:\n",
        "        return np.flipud(np.rot90(img, k=3))\n",
        "\n",
        "np.random.seed(0)\n",
        "train_target = np.zeros((128*1600,40,40), dtype='float32')\n",
        "#train_input = np.zeros((128*1600,40,40), dtype='float32')\n",
        "for i in range(400):\n",
        "    name_img = './drive/MyDrive/train400/'+str('%03d'%(i+1))+'.png'\n",
        "    img = cv2.imread(name_img, cv2.IMREAD_GRAYSCALE)\n",
        "    patch = image.extract_patches_2d(img, (40, 40), 512)\n",
        "    patch = patch.astype('float32')\n",
        "    train_target[512*(i):512*(i+1),:,:] = patch\n",
        "for i in range (128*1600):\n",
        "    x = train_target[i]\n",
        "    train_target[i] = data_aug(x, mode=np.random.randint(0,8))\n",
        "    #train_input[i] = train_target[i] + np.random.normal(0, 15, train_target[i].shape)\n",
        "train_target = train_target.reshape((128*1600,40,40,1))\n",
        "#train_input = train_input.reshape((128*1600,40,40,1))\n",
        "np.random.seed(None)\n",
        "\n",
        "def train_datagen(epoch_num=100,batch_size=128):\n",
        "    sigma = 15\n",
        "    indices = list(range(128*1600))\n",
        "    for _ in range(epoch_num):\n",
        "        np.random.shuffle(indices)    # shuffle\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch_x = train_target[indices[i:i+batch_size]]\n",
        "            noise =  np.random.normal(0, sigma, batch_x.shape)    # noise\n",
        "            batch_y = batch_x + noise\n",
        "            #batch_y = train_input[indices[i:i+batch_size]]\n",
        "            batch_x /= 255\n",
        "            batch_y /= 255\n",
        "            yield batch_y, batch_x\n",
        "\n",
        "def naive_inception_module(layer_in, f1, f2, f3):\n",
        "\t# 1x1 conv\n",
        "\tconv1 = Conv2D(f1, (1,1), padding='same', activation='elu')(layer_in)\n",
        "\t# 3x3 conv\n",
        "\tconv3 = Conv2D(f2, (3,3), padding='same', activation='elu')(layer_in)\n",
        "\t# 5x5 conv\n",
        "\tconv5 = Conv2D(f3, (5,5), padding='same', activation='elu')(layer_in)\n",
        "\t# 3x3 max pooling\n",
        "\tpool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
        "\t# concatenate filters, assumes filters/channels last\n",
        "\tlayer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\treturn layer_out\n",
        "\n",
        "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
        "\t# 1x1 conv\n",
        "\tconv1 = Conv2D(f1, (1,1), padding='same', activation='elu')(layer_in)\n",
        "\t# 3x3 conv\n",
        "\tconv3 = Conv2D(f2_in, (1,1), padding='same', activation='elu')(layer_in)\n",
        "\tconv3 = Conv2D(f2_out, (3,3), padding='same', activation='elu')(conv3)\n",
        "\t# 5x5 conv\n",
        "\tconv5 = Conv2D(f3_in, (1,1), padding='same', activation='elu')(layer_in)\n",
        "\tconv5 = Conv2D(f3_out, (5,5), padding='same', activation='elu')(conv5)\n",
        "\t# 3x3 max pooling\n",
        "\tpool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
        "\tpool = Conv2D(f4_out, (1,1), padding='same', activation='elu')(pool)\n",
        "\t# concatenate filters, assumes filters/channels last\n",
        "\tlayer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\treturn layer_out\n",
        "\n",
        "def DnCNN(filters=64,image_channels=1):\n",
        "\n",
        "  layer_count = 0\n",
        "  inpt = Input(shape=(None,None,image_channels),name = 'input'+str(layer_count))\n",
        "  #layer_count += 1\n",
        "  #x = Conv2D(filters=int(1*filters), kernel_size=(5,5), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=1, use_bias = False,name = 'conv'+str(layer_count))(inpt)\n",
        "  layer_count += 1\n",
        "  x = naive_inception_module(inpt, 64, 128, 32)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=int(1*filters), kernel_size=(3,3), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=2, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.000, name = 'bn'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Activation('elu',name = 'elu'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=int(1*filters), kernel_size=(5,5), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=1, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.000, name = 'bn'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Activation('elu',name = 'elu'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=int(1*filters), kernel_size=(3,3), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=1, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.000, name = 'bn'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Activation('elu',name = 'elu'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=int(1*filters), kernel_size=(5,5), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=1, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.000, name = 'bn'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Activation('elu',name = 'elu'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=int(1*filters), kernel_size=(3,3), strides=(1,1),kernel_initializer='Orthogonal', padding='same', dilation_rate=2, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = BatchNormalization(axis=3, momentum=0.0,epsilon=0.000, name = 'bn'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Activation('elu',name = 'elu'+str(layer_count))(x)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = inception_module(x, 64, 96, 128, 16, 32, 32)\n",
        "  layer_count += 1\n",
        "  x = Add(name = 'add' + str(layer_count))([inpt, x])\n",
        "  layer_count += 1\n",
        "  x = Conv2D(filters=image_channels, kernel_size=(3,3), strides=(1,1), kernel_initializer='Orthogonal',padding='same', dilation_rate=1, use_bias = False,name = 'conv'+str(layer_count))(x)\n",
        "  #layer_count += 1\n",
        "  #x = Subtract(name = 'subtract' + str(layer_count))([inpt, x])   # input - noise\n",
        "  model = Model(inputs=inpt, outputs=x)\n",
        "  return model\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 1e-3\n",
        "    if epoch<=30:\n",
        "        lr = initial_lr\n",
        "    elif epoch<=60:\n",
        "        lr = initial_lr/10\n",
        "    elif epoch<=100:\n",
        "        lr = initial_lr/20\n",
        "    else:\n",
        "        lr = initial_lr/40\n",
        "    initial_lr = 1e-4\n",
        "    t = epoch - 1\n",
        "    #k = 0.1409745975 #50epoch\n",
        "    k = 0.0232584353 #100epoch\n",
        "    lr = initial_lr * math.exp(-k*t)\n",
        "    lr = 1e-4\n",
        "    return lr\n",
        "\n",
        "save_dir = os.path.join('./drive/MyDrive/models')\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "def findLastCheckpoint(save_dir):\n",
        "    file_list = glob.glob(os.path.join(save_dir,'model_*.hdf5'))  # get name list of all .hdf5 files\n",
        "    if file_list:\n",
        "        epochs_exist = []\n",
        "        for file_ in file_list:\n",
        "            result = re.findall(\".*model_(.*).hdf5.*\",file_)\n",
        "            #print(result[0])\n",
        "            epochs_exist.append(int(result[0]))\n",
        "        initial_epoch=max(epochs_exist)\n",
        "    else:\n",
        "        initial_epoch = 0\n",
        "    return initial_epoch\n",
        "\n",
        "\n",
        "initial_epoch = findLastCheckpoint(save_dir=save_dir)\n",
        "if initial_epoch > 0:\n",
        "    print('resuming by loading epoch %03d'%initial_epoch)\n",
        "    model = load_model(os.path.join(save_dir,'model_%03d.hdf5'%initial_epoch), compile=False)\n",
        "else:\n",
        "    model = DnCNN(filters=48,image_channels=1)\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.001), loss=keras.losses.mse)\n",
        "model.summary()\n",
        "checkpointer = ModelCheckpoint(os.path.join(save_dir,'model_{epoch:03d}.hdf5'), verbose=1, save_weights_only=False, save_freq=\"epoch\")\n",
        "csv_logger = CSVLogger(os.path.join(save_dir,'log.csv'), append=True, separator=',')\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "network_history = model.fit(train_datagen(epoch_num=500,batch_size=128), steps_per_epoch=1600, epochs=500, verbose=1, initial_epoch=initial_epoch,\n",
        "                            callbacks=[checkpointer, csv_logger, lr_scheduler])\n",
        "\n",
        "images = []\n",
        "for i in range(1,69):\n",
        "    name_img = './drive/MyDrive/Set68/test ('+str(i)+').png'\n",
        "    img = cv2.imread(name_img, cv2.IMREAD_GRAYSCALE)\n",
        "    j,k = img.shape\n",
        "    img = img.reshape((1,j,k,1))\n",
        "    images.append(img)\n",
        "\n",
        "ite = 10\n",
        "psnrs = []\n",
        "for _ in range(ite):\n",
        "    psnr = []\n",
        "    for i in range(68):\n",
        "        test_target = images[i]\n",
        "        test_input = test_target + np.random.normal(0, 15, test_target.shape)\n",
        "        test_input = test_input.astype('float32')\n",
        "        test_input /= 255\n",
        "        test_output = model.predict(test_input)\n",
        "        test_output *= 255\n",
        "        test_output = test_output*(test_output>=0) - test_output*(test_output>255) + 255*(test_output>255)\n",
        "        #test_output = np.around(test_output)\n",
        "        #test_output = test_output.astype('uint8')\n",
        "        ii,j,m,n = test_target.shape\n",
        "        test_target = test_target.reshape((j,m))\n",
        "        test_output = test_output.reshape((j,m))\n",
        "        psnr_x = peak_signal_noise_ratio(test_target, test_output, data_range=255)\n",
        "        psnr.append(psnr_x)\n",
        "    ave = np.mean(psnr)\n",
        "    psnrs.append(ave)\n",
        "    #print(ave)\n",
        "print(\"Denoising result on testset is\",np.mean(psnrs),\"+-\",np.std(psnrs))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}